setwd("C:/Users/merie/OneDrive/Desktop/iac-mining")

library(readxl)

# Load Mirantis CSV
data <- read.csv("IST_MIR.csv")

# Check structure
colnames(data)
head(data)

# i loaded the rest of the data 

✅ Step 1: Select features + label

# Drop metadata, keep only features + label
X <- data[, c("URL", "File", "Require", "Ensure", "Include", "Attribute",
              "Hard_coded_string", "Command", "File_mode", "SSH_KEY",
              "Lines_of_code", "Comment")]

y <- as.factor(data$defect_status)

head(X)
head(y)


✅ Step 2: Log transform (to normalize big numbers)

X_log <- log1p(X)  # log(1+x) handles zeros safely


✅ Step 3: PCA
pca <- prcomp(X_log, scale. = TRUE)
summary(pca)   # shows how many components explain ~95% of variance



########### and then i just did this log and OCA at once :

mirantis <- read.csv("IST_MIR.csv")
mozilla  <- read.csv("IST_MOZ.csv")
openstack <- read.csv("IST_OST.csv")
wikimedia <- read.csv("IST_WIK.csv")

# work on Mirantis
X_mir <- mirantis[, c("URL", "File", "Require", "Ensure", "Include", "Attribute",
                      "Hard_coded_string", "Command", "File_mode", "SSH_KEY",
                      "Lines_of_code", "Comment")]

y_mir <- as.factor(mirantis$defect_status)

# Log transform
X_log_mir <- log1p(X_mir)

# PCA
pca_mir <- prcomp(X_log_mir, scale. = TRUE)
summary(pca_mir)



######now we will be training the llms 10x10 folds
library(caret)

set.seed(123)

# Combine PCA features with label
data_mir <- data.frame(pca_mir$x[, 1:7], y_mir)  # 7 PCs from PCA

# 10x10 cross-validation
train_control <- trainControl(method = "repeatedcv", 
                              number = 10, 
                              repeats = 10, 
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary)

# Logistic Regression
model_lr <- train(y_mir ~ ., data = data_mir,
                  method = "glm",
                  family = "binomial",
                  metric = "ROC", 
                  trControl = train_control)

print(model_lr)
 had issue with labeling the mir factors


# Fix factor levels for y_mir
y_mir <- factor(y_mir, levels = c(0,1), labels = c("No", "Yes"))

# Recreate dataset with fixed y_mir
data_mir <- data.frame(pca_mir$x[, 1:7], y_mir)


##### train agaiiiin

model_lr <- train(y_mir ~ ., data = data_mir,
                  method = "glm",
                  family = "binomial",
                  metric = "ROC", 
                  trControl = train_control)

print(model_lr)




# Example for Wikimedia
y_wik <- factor(y_wik, levels = c(0,1), labels = c("No", "Yes"))
data_wik <- data.frame(pca_wik$x[, 1:7], y_wik)

model_lr_wik <- train(y_wik ~ ., data = data_wik,
                      method = "glm",
                      family = "binomial",
                      metric = "ROC",
                      trControl = train_control)

print(model_lr_wik)

################################################
##########################################
#######let us start training cart first lets start mirantis and i want to get all the metrcis and every result i need at the end ##############################
########################################
###########################################

# Load libraries
library(caret)
library(pROC)
library(MLmetrics)

# Custom summary to get all metrics
mySummary <- function(data, lev = NULL, model = NULL) {
  precision <- Precision(data$obs, data$pred, positive = "Yes")
  recall    <- Recall(data$obs, data$pred, positive = "Yes")
  f1        <- F1_Score(data$obs, data$pred, positive = "Yes")
  acc       <- mean(data$obs == data$pred)
  roc_auc   <- tryCatch({
    roc_obj <- roc(response = data$obs, predictor = data$Yes)
    auc(roc_obj)
  }, error = function(e) NA)
  
  out <- c(Accuracy = acc,
           Precision = precision,
           Recall = recall,
           F1 = f1,
           ROC = roc_auc)
  return(out)
}

set.seed(123)

# Cross-validation setup
train_control <- trainControl(method = "repeatedcv", 
                              number = 10, 
                              repeats = 10, 
                              classProbs = TRUE, 
                              summaryFunction = mySummary,
                              savePredictions = "final")

# Train CART
model_cart <- train(y_mir ~ ., data = data_mir,
                    method = "rpart",
                    metric = "ROC",
                    trControl = train_control)

# Print summary results
print(model_cart)

# Export metrics
results_cart <- model_cart$results
write.csv(results_cart, "mirantis_cart_results.csv", row.names = FALSE)

# Confusion matrix
preds <- predict(model_cart, data_mir)
conf_mat <- confusionMatrix(preds, data_mir$y_mir)
print(conf_mat)
write.csv(as.data.frame(conf_mat$table), "mirantis_cart_confusion_matrix.csv")

# ROC curve
probs <- predict(model_cart, data_mir, type = "prob")
roc_obj <- roc(data_mir$y_mir, probs$Yes)
plot(roc_obj, col = "red", main = "ROC Curve - Mirantis CART")





####################### mozella training CART
###############################
# Load Mozilla dataset
mozilla <- read.csv("IST_MOZ.csv")

# Features + label
X_moz <- mozilla[, c("URL", "File", "Require", "Ensure", "Include", "Attribute",
                     "Hard_coded_string", "Command", "File_mode", "SSH_KEY",
                     "Lines_of_code", "Comment")]
y_moz <- factor(mozilla$defect_status, labels = c("No", "Yes"))

# Log transform + PCA
X_log_moz <- log1p(X_moz)
pca_moz <- prcomp(X_log_moz, scale. = TRUE)

# Combine PCA features with label (pick enough PCs for ~95% variance, say first 7 like Mirantis)
data_moz <- data.frame(pca_moz$x[, 1:7], y_moz)

set.seed(123)

# Train CART
model_cart_moz <- train(y_moz ~ ., data = data_moz,
                        method = "rpart",
                        metric = "ROC",
                        trControl = train_control)

# Print summary results
print(model_cart_moz)

# Export metrics
results_cart_moz <- model_cart_moz$results
write.csv(results_cart_moz, "mozilla_cart_results.csv", row.names = FALSE)

# Confusion matrix
preds_moz <- predict(model_cart_moz, data_moz)
conf_mat_moz <- confusionMatrix(preds_moz, data_moz$y_moz)
print(conf_mat_moz)
write.csv(as.data.frame(conf_mat_moz$table), "mozilla_cart_confusion_matrix.csv")

# ROC curve
probs_moz <- predict(model_cart_moz, data_moz, type = "prob")
roc_obj_moz <- roc(data_moz$y_moz, probs_moz$Yes)
plot(roc_obj_moz, col = "blue", main = "ROC Curve - Mozilla CART")







# Load OpenStack dataset
openstack <- read.csv("IST_OST.csv")

# Features + label
X_ost <- openstack[, c("URL", "File", "Require", "Ensure", "Include", "Attribute",
                       "Hard_coded_string", "Command", "File_mode", "SSH_KEY",
                       "Lines_of_code", "Comment")]
y_ost <- factor(openstack$defect_status, labels = c("No", "Yes"))

# Log transform + PCA
X_log_ost <- log1p(X_ost)
pca_ost <- prcomp(X_log_ost, scale. = TRUE)

# Combine PCA features with label (choose first 7 PCs like before)
data_ost <- data.frame(pca_ost$x[, 1:7], y_ost)

set.seed(123)

# Train CART
model_cart_ost <- train(y_ost ~ ., data = data_ost,
                        method = "rpart",
                        metric = "ROC",
                        trControl = train_control)

# Print summary results
print(model_cart_ost)

# Export metrics
results_cart_ost <- model_cart_ost$results
write.csv(results_cart_ost, "openstack_cart_results.csv", row.names = FALSE)

# Confusion matrix
preds_ost <- predict(model_cart_ost, data_ost)
conf_mat_ost <- confusionMatrix(preds_ost, data_ost$y_ost)
print(conf_mat_ost)
write.csv(as.data.frame(conf_mat_ost$table), "openstack_cart_confusion_matrix.csv")

# ROC curve
probs_ost <- predict(model_cart_ost, data_ost, type = "prob")
roc_obj_ost <- roc(data_ost$y_ost, probs_ost$Yes)
plot(roc_obj_ost, col = "green", main = "ROC Curve - OpenStack CART")
